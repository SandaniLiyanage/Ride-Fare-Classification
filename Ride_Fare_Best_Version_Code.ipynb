{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ride Fare Best Version Code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOX1HBNA6BwG",
        "colab_type": "text"
      },
      "source": [
        "## 0. Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "el92AualhO86",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c59fe726-022e-4d54-b670-08222bb0e43d"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "from google.colab import drive  \n",
        "drive.mount('/content/gdrive')\n",
        "train_df= pd.read_csv('/content/gdrive/My Drive/Ride Fare/train.csv', index_col=\"tripid\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiVCHOp7By5Z",
        "colab_type": "text"
      },
      "source": [
        "## 2. Feature PreProcessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwW989j5hYAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df['label_encod'] = train_df['label'].apply(lambda x: 0 if x == 'incorrect' else 1 if x == 'correct' else 2)\n",
        "train_df['label_encod'].unique()\n",
        "train_df.drop(['label'],axis=1, inplace=True)\n",
        "\n",
        "def haversine_distance(row):\n",
        "    lat_p, lon_p = row['pick_lat'], row['pick_lon']\n",
        "    lat_d, lon_d = row['drop_lat'], row['drop_lon']\n",
        "    radius = 6371 # km\n",
        "\n",
        "    dlat = np.radians(lat_d - lat_p)\n",
        "    dlon = np.radians(lon_d - lon_p)\n",
        "    a = np.sin(dlat/2) * np.sin(dlat/2) + np.cos(np.radians(lat_p)) * np.cos(np.radians(lat_d)) * np.sin(dlon/2) * np.sin(dlon/2)\n",
        "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
        "    distance = radius * c\n",
        "\n",
        "    return distance\n",
        "\n",
        "train_df['distance'] = train_df.apply(haversine_distance, axis = 1)\n",
        "train_df =  train_df.drop(train_df[['pick_lat','pick_lon','drop_lat','drop_lon']], axis=1)\n",
        "\n",
        "import datetime as dt\n",
        "train_df['pickup_time']= pd.to_datetime(train_df['pickup_time'])\n",
        "train_df['drop_time']= pd.to_datetime(train_df['drop_time'])\n",
        "train_df['AssumedDuration'] = (train_df['drop_time']-train_df['pickup_time']).dt.total_seconds()\n",
        "train_df['duration'] = train_df.apply(\n",
        "    lambda row: row['AssumedDuration'] if np.isnan(row['duration']) else row['duration'],\n",
        "    axis=1\n",
        ")\n",
        "train_df =  train_df.drop(train_df[['AssumedDuration','pickup_time','drop_time']], axis=1)\n",
        "features_df =  train_df.drop('label_encod', axis=1)\n",
        "labels_df = pd.DataFrame(data = train_df['label_encod'], columns = ['label_encod'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mnn4e6YRB-QC",
        "colab_type": "text"
      },
      "source": [
        "## 3. Model Creation and Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbUKeCI1hlpA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numeric_cols = ['additional_fare', 'duration', 'meter_waiting', 'meter_waiting_fare', 'meter_waiting_till_pickup', 'fare', 'distance']\n",
        "\n",
        "numeric_preprocessing_steps = Pipeline(steps = [\n",
        "    ('standard_scaler', StandardScaler()),\n",
        "    ('imputer', SimpleImputer(strategy='mean'))])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers = [\n",
        "        ('num', numeric_preprocessing_steps, numeric_cols)\n",
        "    ],\n",
        "    remainder = \"drop\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhQIawgsjtyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "# get the models to evaluate\n",
        "level0 = list()\n",
        "level0.append(('xg',XGBClassifier(n_estimators=500,subsample=0.14))) \n",
        "level0.append(('mlp',MLPClassifier(hidden_layer_sizes=(50,100,50), max_iter=1000)))\n",
        "level0.append(('dt', RandomForestClassifier(n_estimators = 100,max_features = 'log2')))\n",
        "\n",
        "# define meta learner model\n",
        "level1 = LogisticRegression(penalty=\"l2\", C=3)\n",
        " \n",
        "estimator = StackingClassifier(estimators=level0, final_estimator=level1, cv=10)\n",
        "\n",
        "fullPipe = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', estimator)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stnR949Jk056",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4d1e6fbd-4424-46dc-a3b2-d5caa6da922b"
      },
      "source": [
        "X_train, X_eval, y_train, y_eval = train_test_split(features_df, labels_df, test_size=0.33, shuffle=True, stratify=labels_df, random_state=6)\n",
        "\n",
        "# Train model\n",
        "fullPipe.fit(X_train, np.ravel(y_train))\n",
        "\n",
        "preds = fullPipe.predict(X_eval)\n",
        "y_preds = pd.DataFrame({\"label\": preds},index = y_eval.index)\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "print(f1_score(y_eval, y_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9741362671298978\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xTm0tBQlBKF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "e756a659-68d2-433a-ede4-c464d3192255"
      },
      "source": [
        "fullPipe.fit(features_df, np.ravel(labels_df))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('preprocessor',\n",
              "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
              "                                   sparse_threshold=0.3,\n",
              "                                   transformer_weights=None,\n",
              "                                   transformers=[('num',\n",
              "                                                  Pipeline(memory=None,\n",
              "                                                           steps=[('standard_scaler',\n",
              "                                                                   StandardScaler(copy=True,\n",
              "                                                                                  with_mean=True,\n",
              "                                                                                  with_std=True)),\n",
              "                                                                  ('imputer',\n",
              "                                                                   SimpleImputer(add_indicator=False,\n",
              "                                                                                 copy=True,\n",
              "                                                                                 fill_value=None,\n",
              "                                                                                 missing_values=nan...\n",
              "                                                                        verbose=0,\n",
              "                                                                        warm_start=False))],\n",
              "                                    final_estimator=LogisticRegression(C=3,\n",
              "                                                                       class_weight=None,\n",
              "                                                                       dual=False,\n",
              "                                                                       fit_intercept=True,\n",
              "                                                                       intercept_scaling=1,\n",
              "                                                                       l1_ratio=None,\n",
              "                                                                       max_iter=100,\n",
              "                                                                       multi_class='auto',\n",
              "                                                                       n_jobs=None,\n",
              "                                                                       penalty='l2',\n",
              "                                                                       random_state=None,\n",
              "                                                                       solver='lbfgs',\n",
              "                                                                       tol=0.0001,\n",
              "                                                                       verbose=0,\n",
              "                                                                       warm_start=False),\n",
              "                                    n_jobs=None, passthrough=False,\n",
              "                                    stack_method='auto', verbose=0))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AumdiEIICKN8",
        "colab_type": "text"
      },
      "source": [
        "## 4. Test data PreProcessing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuPktjVnlVDH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df= pd.read_csv('/content/gdrive/My Drive/Ride Fare/test.csv', index_col=\"tripid\")\n",
        "test_df['distance'] = test_df.apply(haversine_distance, axis = 1)\n",
        "test_df =  test_df.drop(test_df[['pick_lat','pick_lon','drop_lat','drop_lon']], axis=1)\n",
        "\n",
        "test_df['pickup_time']= pd.to_datetime(test_df['pickup_time'])\n",
        "test_df['drop_time']= pd.to_datetime(test_df['drop_time'])\n",
        "\n",
        "test_df['AssumedDuration'] = (test_df['drop_time']-test_df['pickup_time']).dt.total_seconds()\n",
        "test_df['duration'] = test_df.apply(lambda row: row['AssumedDuration'] if np.isnan(row['duration']) else row['duration'],axis=1)\n",
        "test_df =  test_df.drop('AssumedDuration', axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "USoJlXJHnKJ6"
      },
      "source": [
        "## 5. Make Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piE_Td4vm86m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_preds = fullPipe.predict(test_df)\n",
        "\n",
        "submission_df = pd.read_csv('/content/gdrive/My Drive/Ride Fare/sample_submission.csv', index_col=\"tripid\")\n",
        "\n",
        "# Make sure we have the rows in the same order\n",
        "np.testing.assert_array_equal(test_df.index.values, submission_df.index.values)\n",
        "\n",
        "# Save predictions to submission data frame\n",
        "submission_df['prediction'] = test_preds\n",
        "\n",
        "submission_df.to_csv('/content/gdrive/My Drive/Ride Fare/my_submission_stack3.csv', index=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}